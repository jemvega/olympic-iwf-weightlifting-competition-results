{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_list = [1996 + x for x in range(24)]\n",
    "\n",
    "olympic_years_list = [1980 + x for x in range(0, 40, 4)]\n",
    "\n",
    "country_codes = pd.read_html(\"https://www.iban.com/country-codes\")[0]\n",
    "\n",
    "class CheckFunctions():\n",
    "    \n",
    "    @staticmethod\n",
    "    def check_group(results_dataframe):\n",
    "        try: \n",
    "            if not \"Group\\n\" in results_dataframe.iloc[0].values:\n",
    "                results_dataframe.insert(2, \"Group\", \"A\")\n",
    "                return results_dataframe\n",
    "            else: \n",
    "                return results_dataframe\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    @staticmethod\n",
    "    def check_bodyweight(results_dataframe):\n",
    "        try:\n",
    "            if (not \"Bodyweight\\n\" or not \"Body weight\\n\") in results_dataframe.iloc[0].values:\n",
    "                results_dataframe.insert(3, \"Body Weight (kg)\", \"NaN\")\n",
    "                return results_dataframe\n",
    "            else:\n",
    "                return results_dataframe\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    @staticmethod\n",
    "    def check_nation(results_dataframe):\n",
    "        try:\n",
    "            if not \"Nation\\n\" in results_dataframe.iloc[0].values:\n",
    "                new_cols = results_dataframe[1].str.split(\"(\", 1, expand=True)\n",
    "                results_dataframe[1] = new_cols[0]\n",
    "                results_dataframe.insert(2, \"Nationality\", new_cols[1])\n",
    "                results_dataframe[\"Nationality\"] = results_dataframe[\"Nationality\"].str.strip(\")\")\n",
    "                return results_dataframe\n",
    "            else:\n",
    "                return results_dataframe\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    @staticmethod\n",
    "    def check_max_lift(results_dataframe):\n",
    "        try:\n",
    "            if not \"Result\\n\" in results_dataframe.iloc[1].values:\n",
    "                results_dataframe.insert(8, \"Max Snatch\", 0)\n",
    "                results_dataframe.insert(13, \"Max C/J\", 0)\n",
    "                return results_dataframe\n",
    "            else:\n",
    "                return results_dataframe\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    @staticmethod\n",
    "    def check_rank(results_dataframe):\n",
    "        try:\n",
    "            if not \"Rank\\n\" in results_dataframe.iloc[1].values:\n",
    "                results_dataframe.insert(9, \"Snatch Rank\", 0)\n",
    "                results_dataframe.insert(14, \"C/J Rank\", 0)\n",
    "                return results_dataframe\n",
    "            else:\n",
    "                results_dataframe[\"Snatch Rank\"] = results_dataframe[9]\n",
    "                results_dataframe[\"C/J Rank\"] = results_dataframe[14]\n",
    "                results_dataframe.drop(columns=[8, 14], inplace = True)\n",
    "                return results_dataframe\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "class WikiParser():\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_h1_text(website_url):\n",
    "        url = requests.get(website_url)\n",
    "        page = BeautifulSoup(url.text, 'lxml')\n",
    "        header_id = page.find(\"h1\", id=\"firstHeading\").get_text()\n",
    "        return header_id\n",
    "    \n",
    "    @staticmethod\n",
    "    # Create a function to grab all of the results urls from each weight class of the competition year\n",
    "    def process_urls(url):\n",
    "        website_url = requests.get(url)\n",
    "        page = BeautifulSoup(website_url.text, \"lxml\")\n",
    "        results_urls = []\n",
    "        for link in page.find_all(\"a\", attrs={\"href\": re.compile(f\"^{url[24:]}_\")}):\n",
    "            results_urls.append(link.get('href'))\n",
    "        return results_urls\n",
    "    \n",
    "    @staticmethod\n",
    "    def iwf_competitions_to_dataframe(website_url, header_name):\n",
    "        # parse html; find table; find table rows\n",
    "        url = requests.get(website_url)\n",
    "        page = BeautifulSoup(url.text, 'lxml')\n",
    "        header_id = page.find(\"span\", id=header_name)\n",
    "        wiki_table = header_id.parent.find_next_sibling(\"table\", class_=\"wikitable\")\n",
    "        table_cells = wiki_table.find('td')\n",
    "        table_rows = wiki_table.find_all(\"tr\")\n",
    "        # Iterate through rows and load text into a list\n",
    "        results_table = []\n",
    "        for row in table_rows:\n",
    "            table_tags = row.find_all([\"th\", \"td\"])\n",
    "            td = list(item.text for item in table_tags)\n",
    "            results_table.append(td)\n",
    "        results_df = pd.DataFrame(results_table)\n",
    "        return results_df\n",
    "    \n",
    "    @staticmethod\n",
    "    def iwf_links(website_url, header_name, years_list):\n",
    "        # parse html; find table; find table rows\n",
    "        url = requests.get(website_url)\n",
    "        page = BeautifulSoup(url.text, 'lxml')\n",
    "        header_id = page.find(\"span\", id=header_name)\n",
    "        wiki_table = header_id.parent.find_next_sibling(\"table\", class_=\"wikitable\")\n",
    "        # Iterate through rows and load text into a list\n",
    "        links_list = []\n",
    "        for link in wiki_table.find_all(\"a\"):\n",
    "            for year in years_list:\n",
    "                if f\"{year}\" in link.get_text():\n",
    "                    links_list.append(link.get(\"href\"))\n",
    "        return links_list\n",
    "    \n",
    "    @staticmethod\n",
    "    def oly_links(website_url, div_id, years_list):\n",
    "        # parse html; find table; find table rows\n",
    "        url = requests.get(website_url)\n",
    "        page = BeautifulSoup(url.text, 'lxml')\n",
    "        page_details = page.find(\"div\", id=div_id)\n",
    "        oly_years_table = page_details.find(\"table\", class_=\"infobox\")\n",
    "        # Iterate through rows and load text into a list\n",
    "        links_list = []\n",
    "        for link in oly_years_table.find_all(\"a\"):\n",
    "            for year in olympic_years_list:\n",
    "                if f\"{year}\" in link.get_text():\n",
    "                    links_list.append(link.get(\"href\"))\n",
    "        return links_list\n",
    "    \n",
    "    @staticmethod\n",
    "    def results_to_dataframe(website_url, header_name):\n",
    "        # parse html; find table; find table rows\n",
    "        url = requests.get(website_url)\n",
    "        page = BeautifulSoup(url.text, 'lxml')\n",
    "        header_id = page.find(\"span\", id=header_name)\n",
    "        wiki_table = header_id.parent.find_next_sibling(\"table\", class_=\"wikitable\")\n",
    "        table_cells = wiki_table.find('td')\n",
    "        table_rows = wiki_table.find_all(\"tr\")\n",
    "        # Iterate through rows and load text into a list\n",
    "        results_table = []\n",
    "        for elem in table_rows:\n",
    "            strikethrough = elem.find_all(\"s\")\n",
    "            bold = elem.find_all(\"b\")\n",
    "            for string in strikethrough:\n",
    "                negative = \"\".join(f\"-{string.get_text()}\")\n",
    "                string.replace_with(negative)\n",
    "            for string in bold:\n",
    "                non_bold = \"\".join(f\"{string.get_text()}\")\n",
    "                string.replace_with(non_bold)\n",
    "        for tag in table_rows:\n",
    "            table_tags = tag.find_all([\"th\", \"td\"])\n",
    "            td = []\n",
    "            for text in table_tags:\n",
    "                td.append(text.get_text())\n",
    "            results_table.append(td)\n",
    "        results_df = pd.DataFrame(results_table)\n",
    "        return results_df\n",
    "    \n",
    "class ResultsCleanup():\n",
    "    \n",
    "    @staticmethod\n",
    "    def column_row_cleanup(results_dataframe):\n",
    "        results_dataframe = CheckFunctions.check_group(results_dataframe)\n",
    "        results_dataframe = CheckFunctions.check_bodyweight(results_dataframe)\n",
    "        results_dataframe = CheckFunctions.check_nation(results_dataframe)\n",
    "        results_dataframe = CheckFunctions.check_max_lift(results_dataframe)\n",
    "        results_dataframe = CheckFunctions.check_rank(results_dataframe)\n",
    "        column_names = (\n",
    "                \"Comp Rank, Athlete Name, Nationality, Group, Body Weight (kg), \"\n",
    "                \"Snatch 1 (kg), Snatch 2 (kg), Snatch 3 (kg), Max Snatch, Snatch Rank, \"\n",
    "                \"C/J 1 (kg), C/J 2 (kg), C/J 3 (kg), Max C/J, C/J Rank, Total\"\n",
    "                        ).split(\", \")\n",
    "        results_dataframe.columns = column_names\n",
    "        results_dataframe.drop([0,1], inplace=True)\n",
    "        results_dataframe.reset_index(inplace=True)\n",
    "        results_dataframe.drop(\"index\", axis=1, inplace=True)\n",
    "        # Change country name to country code for consistency\n",
    "        for country in results_dataframe[\"Nationality\"].values.tolist():\n",
    "            if country in country_codes[\"Country\"].values.tolist():\n",
    "                index = country_codes[\"Country\"].values.tolist().index(country)\n",
    "                code = country_codes[\"Alpha-3 code\"][index]\n",
    "                results_dataframe[\"Nationality\"][index] = code\n",
    "            else:\n",
    "                pass\n",
    "        return results_dataframe\n",
    "    \n",
    "    @staticmethod\n",
    "    def check_float(string):\n",
    "        try:\n",
    "            float(string)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "    \n",
    "    @staticmethod\n",
    "    def string_to_float(converted_list):\n",
    "        for elem in converted_list:\n",
    "            if ResultsCleanup.check_float(elem):\n",
    "                rational = float(elem)\n",
    "                index = converted_list.index(elem)\n",
    "                converted_list.pop(index)\n",
    "                converted_list.insert(index, rational)\n",
    "            elif ResultsCleanup.check_float(elem) == False:\n",
    "                index = converted_list.index(elem)\n",
    "                converted_list.pop(index)\n",
    "                converted_list.insert(index, 0)\n",
    "            else:\n",
    "                print(\"error\")\n",
    "        return converted_list\n",
    "    \n",
    "    @staticmethod\n",
    "    def lift_rankings(results_dataframe, lift_col_names, max_lift, lift_rank):\n",
    "        \"\"\"Note: lift_col_names = [\"Snatch 1 (kg)\", \"Snatch 2 (kg)\", \"Snatch 3 (kg)\"] \n",
    "                                or [\"C/J 1 (kg)\", \"C/J 2 (kg)\", \"C/J 3 (kg)\"]\n",
    "        Note: max_lift = 'Max Snatch' or 'Max C/J' \n",
    "        Note: lift_rank = 'Snatch Rank' or 'C/J Rank'\"\"\"\n",
    "        temp_list = results_dataframe[lift_col_names].values.tolist()\n",
    "        max_weight = []\n",
    "        for elem in temp_list:\n",
    "            ResultsCleanup.string_to_float(elem)\n",
    "        results_dataframe[lift_col_names] = temp_list\n",
    "        for row in temp_list:\n",
    "            row.sort()\n",
    "        for row in range(len(temp_list)):\n",
    "            max_weight.append(temp_list[row][-1])\n",
    "        # Sort the indices of the max lifts to get comp rank (overall place)\n",
    "        max_lift_rankings = list(sorted(max_weight, reverse=True).index(num) + 1 for num in max_weight)\n",
    "        results_dataframe[max_lift] = max_weight\n",
    "        results_dataframe[lift_rank] = max_lift_rankings\n",
    "        return results_dataframe\n",
    "\n",
    "    @staticmethod\n",
    "    def data_cleanup(results_dataframe):\n",
    "        podium = [1, 2, 3]\n",
    "        for i in range(len(podium)):\n",
    "            results_dataframe[\"Comp Rank\"][i] = podium[i]\n",
    "        results_dataframe[\"Body Weight (kg)\"] = ResultsCleanup.string_to_float(results_dataframe[\"Body Weight (kg)\"].values.tolist())\n",
    "        results_dataframe[\"Comp Rank\"] = ResultsCleanup.string_to_float(results_dataframe[\"Comp Rank\"].values.tolist())\n",
    "        results_dataframe[\"Total\"] = ResultsCleanup.string_to_float(results_dataframe[\"Total\"].values.tolist())\n",
    "        results_dataframe = results_dataframe.replace(\"\\n\", \"\", regex=True)\n",
    "        results_dataframe.replace(\"\\xa0\", \"\", regex=True, inplace=True)\n",
    "        results_dataframe.fillna(\"NaN\", inplace = True)\n",
    "        return results_dataframe\n",
    "\n",
    "class datatable_cleanup():\n",
    "    \n",
    "    @staticmethod\n",
    "    def insert_year(website_url):\n",
    "        if website_url[30:43] == \"Weightlifting\":\n",
    "            year = website_url[51:55]\n",
    "            return year\n",
    "        else:\n",
    "            year = website_url[30:34]\n",
    "            return year \n",
    "    \n",
    "    @staticmethod\n",
    "    def insert_gender(website_url):\n",
    "        if website_url[79:82] == \"Men\":\n",
    "            gender = \"M\"\n",
    "            return gender\n",
    "        elif website_url[79:84] == \"Women\":\n",
    "            gender = \"W\"\n",
    "            return gender\n",
    "        elif website_url[82:85] == \"Men\":\n",
    "            gender = \"M\"\n",
    "            return gender\n",
    "        elif website_url[82:87] == \"Women\":\n",
    "            gender = \"W\"\n",
    "            return gender\n",
    "            \n",
    "    @staticmethod\n",
    "    def results_table(website_url):\n",
    "        try:\n",
    "            year = datatable_cleanup.insert_year(website_url)\n",
    "            gender = datatable_cleanup.insert_gender(website_url)\n",
    "            url_header = WikiParser.get_h1_text(website_url)\n",
    "            header_name = \"Results\"\n",
    "            snatch_cols = [\"Snatch 1 (kg)\", \"Snatch 2 (kg)\", \"Snatch 3 (kg)\"] \n",
    "            clean_cols = [\"C/J 1 (kg)\", \"C/J 2 (kg)\", \"C/J 3 (kg)\"]\n",
    "            df = WikiParser.results_to_dataframe(website_url, header_name)\n",
    "            ResultsCleanup.column_row_cleanup(df)\n",
    "            ResultsCleanup.data_cleanup(df)\n",
    "            ResultsCleanup.lift_rankings(df, snatch_cols, \"Max Snatch\", \"Snatch Rank\")\n",
    "            ResultsCleanup.lift_rankings(df, clean_cols, \"Max C/J\", \"C/J Rank\")\n",
    "            df.insert(0,\"Year\", year)\n",
    "            df.insert(1, \"Gender\", gender)\n",
    "            file_name = url_header + \".csv\"\n",
    "            df.to_csv(file_name)\n",
    "            return file_name\n",
    "        except:\n",
    "            return \"Error\"\n",
    "        \n",
    "    @staticmethod\n",
    "    def concat_csv(file_name):\n",
    "        file_pattern = \".csv\"\n",
    "        file_rename = file_name + file_pattern\n",
    "        list_of_files = [file for file in glob(\"*{}\".format(file_pattern))]\n",
    "        # Combine all files in the list into a dataframe\n",
    "        dataframe_csv = pd.concat([pd.read_csv(file, engine=\"python\") for file in list_of_files])\n",
    "        # Export the dataframe to csv\n",
    "        dataframe_csv.to_csv(file_rename, index=False, encoding='utf-8')\n",
    "        list_of_files\n",
    "        return list_of_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook webscraping_functions.ipynb to script\n",
      "[NbConvertApp] Writing 13148 bytes to webscraping_functions.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script webscraping_functions.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
